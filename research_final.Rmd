---
title: "Statistical Analysis of Real/Fake Job Postings"
output: html_document
---

Part 1. The aim of the research.

The goal of this project is to analyze job postings to identify characteristics that distinguish fraudulent postings from legitimate ones. This dataset contains approximately 18,000 job descriptions, of which about 800 are fake.

**Research Questions:**

1.  Do fraudulent job postings offer different salary ranges compared to real ones?
2.  Is there a difference in description length between fake and real job postings?
3.  Are fraudulent postings more likely to have missing company profiles?
4.  Does location (specifically California) affect the probability of a posting being fraudulent?

**Project Outline:**

1.  Data Import and Overview
2.  Data Preprocessing (salary standardization, derived variables)
3.  Exploratory Data Analysis (EDA) with visualizations
4.  Hypothesis Testing (to be completed later)
5.  Logistic Regression Modeling (to be completed later)

Part 2. Overview of data.

```{r}
library(tidyverse)

data <- read.csv("fake_job_postings.csv")
```

Structure:

```{r}
cat("Dataset dimensions:", nrow(data), "rows x", ncol(data), "columns\n")
cat("\nColumn names:\n")
print(colnames(data))
```

```{r}
head(data)
```

The dataset contains the following variables:

| Variable | Description |
|------------------------------------|------------------------------------|
| `job_id` | Unique id of job posting |
| `title` | Title |
| `location` | Location in format "Country, State, City" |
| `department` | Department |
| `salary_range` | Offered salary range (various formats there is even date format) |
| `company_profile` | Text description of the company |
| `description` | Full job description text |
| `requirements` | Requirements |
| `benefits` | Benefits |
| `telecommuting` | Binary: 1 is remote work allowed |
| `has_company_logo` | Binary: 1 is company logo present |
| `has_questions` | Binary: 1 is screening questions present |
| `employment_type` | Full-time, Part-time, Contract, etc. |
| `required_experience` | Experience level required |
| `required_education` | Education level required |
| `industry` | Industry |
| `function` | Job function/role category |
| `fraudulent` | **Target variable**: 1 = fake, 0 = real |

```{r}
cat("Target variable distribution:\n")
table(data$fraudulent)
cat("\nFraudulent rate:", round(mean(data$fraudulent) * 100, 2), "%")
```

Part 3: Data preprocessing.

1.  Salary range stantardisation.

The salary_range column has inconsistent formats that need standardization:

-   Full format: "20000-28000" (already correct)
-   Abbreviated: "20-28" (needs conversion to thousands)
-   Zero values: "0-0" (should be treated as missing)
-   Empty/null values

```{r}
cat("Sample of salary_range values:\n")
head(data$salary_range[data$salary_range != ""], 20)
```

```{r}
cat("Non-empty salary ranges:", sum(data$salary_range != ""), "\n")
cat("Empty salary ranges:", sum(data$salary_range == ""), "\n")
```

```{r}
standardize_salary <- function(salary_str) {
  if (is.na(salary_str) || salary_str == "" || salary_str == "0-0") {
    return(c(NA, NA))
  }
  clean <- gsub("[$,\\s]", "", salary_str)
  parts <- str_split(clean, "-")[[1]]

  if (length(parts) != 2) {
    return(c(NA, NA))
  }

  lower <- suppressWarnings(as.numeric(parts[1]))
  upper <- suppressWarnings(as.numeric(parts[2]))
  if (!is.na(lower) && lower == 0) lower <- NA
  if (!is.na(upper) && upper == 0) upper <- NA
  # if < 250: multiply by 1000 (anual likely in 1000-s)
  # if < 3000 multiply by 12 to get annual (likely real monthly salary)
  # Otherwise: already annual salary, keep as is
  if (!is.na(lower) && !is.na(upper)) {
    if (lower < 250 && upper < 250) {
      lower <- lower * 1000
      upper <- upper * 1000
    } else if (lower < 3000 && upper < 3000) {
      lower <- lower * 12
      upper <- upper * 12
    }
  }

  return(c(lower, upper))
}
salary_standardized <- t(sapply(data$salary_range, standardize_salary))
data$salary_lower <- salary_standardized[, 1]
data$salary_upper <- salary_standardized[, 2]
data$salary_mid <- (data$salary_lower + data$salary_upper) / 2
```

```{r}
cat("Salary standardization results:\n")
cat("Valid salary entries:", sum(!is.na(data$salary_mid)), "\n")
cat("Missing salary entries:", sum(is.na(data$salary_mid)), "\n\n")

cat("Salary statistics (midpoint):\n")
summary(data$salary_mid[!is.na(data$salary_mid)])
```

2.  Derive variables for analysis.

Based on the hypotheses in the interim report, we need the following variables:

```{r}
data <- data %>%
  mutate(
    #number of characters
    desc_length = nchar(as.character(description)),

    empty_profile = is.na(company_profile) | company_profile == "",

    # US location indicator
    is_us = grepl("^US", location),

    # California location indicator
    is_california = grepl("^US, CA", location),

    # entry-level experience indicator
    is_entry_level = required_experience %in% c("Entry level", "Not Applicable", "Internship") |
                     required_experience == "",

    # already binary
    has_logo = has_company_logo == 1,
    # for visualization
    salary_category = case_when(
      is.na(salary_mid) ~ "Unknown",
      salary_mid < 20000 ~ "<20k",
      salary_mid < 50000 ~ "20k-50k",
      salary_mid < 100000 ~ "50k-100k",
      salary_mid < 200000 ~ "100k-200k",
      TRUE ~ ">200k"
    ),

    desc_length_category = case_when(
      desc_length < 1000 ~ "<1000",
      desc_length < 4000 ~ "1000-4000",
      TRUE ~ ">4000"
    )
  )

# Convert salary_category to ordered factor
data$salary_category <- factor(data$salary_category,
                                levels = c("<20k", "20k-50k", "50k-100k", "100k-200k", ">200k", "Unknown"))

data$desc_length_category <- factor(data$desc_length_category,
                                     levels = c("<1000", "1000-4000", ">4000"))
```

3.  Splitting data by fraudulent factor

```{r}
fraudulent_data <- data %>% filter(fraudulent == 1)
real_data <- data %>% filter(fraudulent == 0)

n_fake <- nrow(fraudulent_data)
n_real <- nrow(real_data)

cat("Real job postings:", n_real, "\n")
cat("Fraudulent job postings:", n_fake, "\n")
cat("Fraud rate:", round(n_fake / nrow(data) * 100, 2), "%\n")

```

**Part 4. EDA**

Distribution of our target variable

```{r}
ggplot(data, aes(x = factor(fraudulent), fill = factor(fraudulent))) +
  geom_bar() +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5) +
  scale_fill_manual(values = c("0" = "#3498db", "1" = "#e74c3c"),
                    labels = c("Real", "Fraudulent")) +
  labs(title = "1. Ratio of Real (0) and Fraudulent (1) Vacancies",
       x = "Fraudulent (0=No, 1=Yes)",
       y = "Count",
       fill = "Type") +
  theme_minimal() +
  theme(legend.position = "right")
```

The dataset is highly imbalanced with approximately 95% real postings and 5% fraudulent ones. This class imbalance is important to consider for hypothesis testing and modeling.

**Salary distribution by fraud status**

```{r fig.width=10, fig.height=5}
salary_data <- data %>% filter(!is.na(salary_mid) & is_us)

ggplot(salary_data, aes(x = salary_mid, fill = factor(fraudulent))) +
  geom_histogram(bins = 50, alpha = 0.7, position = "identity") +
  scale_fill_manual(values = c("0" = "#3498db", "1" = "#e74c3c"),
                    labels = c("Real", "Fraudulent")) +
  labs(title = "Salary Distribution: Real vs Fraudulent Postings (US Only)",
       x = "Salary (Midpoint)",
       y = "Count",
       fill = "Type") +
  theme_minimal() +
  scale_x_continuous(labels = scales::comma) +
  coord_cartesian(xlim = c(0, 300000))
```

The histogram shows that fraudulent job postings are concentrated in the lowest salary range (0-50,000), as legitimate postings are more evenly distributed across all salary brackets with peaks around 50,000-100,000 and 100,000-200,000. Contrary to the initial hypothesis that scammers lure victims with unrealistically high salaries, the data reveals that most fraudulent postings actually offer lower compensation, likely targeting desperate or inexperienced job seekers. Both distributions are highly right-skewed.

\
Fraud probability by the salary category. (US only)

```{r fig.width=10, fig.height=5}
salary_fraud_prob <- data %>%
  filter(salary_category != "Unknown" & is_us) %>%
  group_by(salary_category) %>%
  summarise(
    total = n(),
    fraudulent_count = sum(fraudulent),
    fraud_probability = mean(fraudulent),
    .groups = "drop"
  )

print(salary_fraud_prob)

ggplot(salary_fraud_prob, aes(x = salary_category, y = fraud_probability, fill = salary_category)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(fraud_probability * 100, 1), "%")), vjust = -0.5) +
  scale_fill_manual(values = c("<20k" = "#9b59b6", "20k-50k" = "#e67e22",
                                "50k-100k" = "#3498db", "100k-200k" = "#1abc9c", ">200k" = "#e74c3c")) +
  labs(title = "Do Scammers Lure You with High Salaries?",
       subtitle = "(Probability of fake depending on the amount - US only)",
       x = "Salary",
       y = "Probability of Fraud") +
  theme_minimal() +
  theme(legend.position = "none")
```

This visualization shows whether fraudulent postings tend to offer unrealistically high salaries to attract victims. The fraud probability follows a U-shaped pattern across salary ranges, with the highest fraud rates occurring at the extremes: very low salaries and very high. The middle salary ranges (50k-200k) show the lowest fraud probability, suggesting these represent the "normal" legitimate job market where fraudulent postings are less common. This reveals that scammers use two distinct strategies: targeting desperate job seekers with extremely low-paying positions and luring ambitious candidates with unrealistically high compensation offers.

**Salary statistics by group**

```{r}
real_data_us <- real_data %>% filter(is_us)
fraudulent_data_us <- fraudulent_data %>% filter(is_us)

cat("REAL postings with salary data:", sum(!is.na(real_data_us$salary_mid)), "\n")
cat("Mean salary:", round(mean(real_data_us$salary_mid, na.rm = TRUE), 2), "\n")
cat("Median salary:", round(median(real_data_us$salary_mid, na.rm = TRUE), 2), "\n")
cat("SD:", round(sd(real_data_us$salary_mid, na.rm = TRUE), 2), "\n\n")

cat("FRAUDULENT postings with salary data:", sum(!is.na(fraudulent_data_us$salary_mid)), "\n")
cat("Mean salary:", round(mean(fraudulent_data_us$salary_mid, na.rm = TRUE), 2), "\n")
cat("Median salary:", round(median(fraudulent_data_us$salary_mid, na.rm = TRUE), 2), "\n")
cat("SD:", round(sd(fraudulent_data_us$salary_mid, na.rm = TRUE), 2), "\n")

```

**Description length analysis.**

1.  Discription length distribution

```{r fig.width=10, fig.height=5}
ggplot(data, aes(x = desc_length, fill = factor(fraudulent))) +
  geom_histogram(bins = 50, alpha = 0.7, position = "identity") +
  scale_fill_manual(values = c("0" = "#3498db", "1" = "#e74c3c"),
                    labels = c("Real", "Fraudulent")) +
  labs(title = "Description Length Distribution: Real vs Fraudulent",
       x = "Description Length (characters)",
       y = "Count",
       fill = "Type") +
  theme_minimal() +
  coord_cartesian(xlim = c(0, 15000))
```

Both real and fraudulent job postings show highly right-skewed distributions with most descriptions concentrated in the 0-2,500 character range. The overlapping distributions in the lower range indicate that description length alone is not a strong discriminator between real and fraudulent postings, though extremely short descriptions appear more suspicious.

2.  Fraud probability be the length of the description.

```{r fig.width=10, fig.height=5}
desc_fraud_prob <- data %>%
  group_by(desc_length_category) %>%
  summarise(
    total = n(),
    fraudulent_count = sum(fraudulent),
    fraud_probability = mean(fraudulent),
    .groups = "drop"
  )

print(desc_fraud_prob)

ggplot(desc_fraud_prob, aes(x = desc_length_category, y = fraud_probability, fill = desc_length_category)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(fraud_probability * 100, 1), "%")), vjust = -0.5) +
  scale_fill_manual(values = c("<1000" = "#3498db", "1000-4000" = "#2ecc71", ">4000" = "#1abc9c")) +
  labs(title = "Do Scammers Write Shorter Texts?",
       subtitle = "(Probability of fake depending on the number of characters)",
       x = "Length of Text",
       y = "Probability of Fraud") +
  theme_minimal() +
  theme(legend.position = "none")
```

The fraud probability shows a clear pattern where shorter job descriptions have a 5.9% fraud rate while very long descriptions (\>4000 characters) have the highest fraud rate at 7.8%, and medium-length descriptions (1000-4000 characters) have the lowest fraud rate at 3.7%. This suggests that both extremely brief and excessively lengthy job descriptions should raise suspicion, with the "sweet spot" for legitimate postings falling in the moderate length range that reflects typical professional job posting standards. The pattern indicates scammers either do minimal effort (short texts) or overcompensate with long descriptions to appear legitimate, while genuine employers typically provide appropriately detailed descriptions in the middle range.

3.  Description length

```{r}
cat("REAL postings:\n")
cat("Mean length:", round(mean(real_data$desc_length, na.rm = TRUE), 2), "characters\n")
cat("Median length:", round(median(real_data$desc_length, na.rm = TRUE), 2), "characters\n")
cat("SD:", round(sd(real_data$desc_length, na.rm = TRUE), 2), "\n\n")

cat("FRAUDULENT postings:\n")
cat("Mean length:", round(mean(fraudulent_data$desc_length, na.rm = TRUE), 2), "characters\n")
cat("Median length:", round(median(fraudulent_data$desc_length, na.rm = TRUE), 2), "characters\n")
cat("SD:", round(sd(fraudulent_data$desc_length, na.rm = TRUE), 2), "\n")
```

**Company profile analysis**

Missing company profile by fraud status

```{r fig.width=10, fig.height=5}
profile_data <- data %>%
  mutate(missing_profile = ifelse(empty_profile, "True", "False")) %>%
  group_by(missing_profile, fraudulent) %>%
  summarise(count = n(), .groups = "drop")

ggplot(profile_data, aes(x = missing_profile, y = count, fill = factor(fraudulent))) +
  geom_bar(stat = "identity", position = "stack") +
  geom_text(aes(label = count), position = position_stack(vjust = 0.5)) +
  scale_fill_manual(values = c("0" = "#1abc9c", "1" = "#e74c3c"),
                    labels = c("Real (0)", "Fraudulent (1)")) +
  labs(title = "3. The Impact of Missing Company Descriptions on Fraud",
       x = "missing_profile",
       y = "Count",
       fill = "fraudulent") +
  theme_minimal()
```

The visualization dramatically shows that missing company profiles are a strong indicator of fraud, with 587 fraudulent postings out of 3,308 total postings with missing profiles compared to only 279 fraudulent postings out of 14,572 postings with profiles present. Postings with missing company descriptions are approximately 9 times more likely to be fraudulent than those with complete profiles, making this one of the most powerful predictors of fraud in the dataset.

Empty profile statistics.

```{r}
cat("REAL postings:\n")
cat("With empty profile:", sum(real_data$empty_profile), "/", n_real,
    "=", round(mean(real_data$empty_profile) * 100, 2), "%\n\n")

cat("FRAUDULENT postings:\n")
cat("With empty profile:", sum(fraudulent_data$empty_profile), "/", n_fake,
    "=", round(mean(fraudulent_data$empty_profile) * 100, 2), "%\n")
```

**Location analysis (specifically California)**

Fraud rate by the location

```{r fig.width=10, fig.height=5}
location_data <- data %>%
  mutate(location_type = ifelse(is_california, "California", "Other")) %>%
  group_by(location_type) %>%
  summarise(
    total = n(),
    fraudulent_count = sum(fraudulent),
    fraud_rate = mean(fraudulent),
    .groups = "drop"
  )

print(location_data)

ggplot(location_data, aes(x = location_type, y = fraud_rate, fill = location_type)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(fraud_rate * 100, 2), "%")), vjust = -0.5) +
  scale_fill_manual(values = c("California" = "#e74c3c", "Other" = "#3498db")) +
  labs(title = "Fraud Rate: California vs Other Locations",
       x = "Location",
       y = "Fraud Rate") +
  theme_minimal() +
  theme(legend.position = "none")
```

Top countries by posting count

```{r fig.width=10, fig.height=6}
data$country <- sapply(strsplit(as.character(data$location), ","), function(x) trimws(x[1]))

country_stats <- data %>%
  group_by(country) %>%
  summarise(
    total = n(),
    fraudulent_count = sum(fraudulent),
    fraud_rate = mean(fraudulent),
    .groups = "drop"
  ) %>%
  filter(total >= 50) %>%  # countries with at least 50 postings
  arrange(desc(total)) %>%
  head(15)

print(country_stats)

ggplot(country_stats, aes(x = reorder(country, total), y = total, fill = fraud_rate)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_fill_gradient(low = "#3498db", high = "#e74c3c") +
  labs(title = "Top 15 Countries by Job Postings",
       x = "Country",
       y = "Number of Postings",
       fill = "Fraud Rate") +
  theme_minimal()
```

We can not assert that this means something as clearly the data was collected mostly and US.

**Employment type analysis**

```{r fig.width=10, fig.height=5}
emp_type_stats <- data %>%
  filter(employment_type != "") %>%
  group_by(employment_type) %>%
  summarise(
    total = n(),
    fraudulent_count = sum(fraudulent),
    fraud_rate = mean(fraudulent),
    .groups = "drop"
  ) %>%
  arrange(desc(total))

print(emp_type_stats)

ggplot(emp_type_stats, aes(x = reorder(employment_type, -fraud_rate), y = fraud_rate, fill = employment_type)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(fraud_rate * 100, 1), "%")), vjust = -0.5, size = 3) +
  labs(title = "Fraud Rate by Employment Type",
       x = "Employment Type",
       y = "Fraud Rate") +
  theme_minimal() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1))
```

Part-time positions show the highest fraud rate at 9.3%, more than double the rate of full-time positions and over three times higher than contract positions, suggesting scammers preferentially target job seekers looking for flexible or supplemental income opportunities. The "Other" category has a 6.6% fraud rate, while temporary positions show the lowest fraud rate at just 0.8%, indicating that more formal, structured employment arrangements are less susceptible to postings. This pattern suggests that fraudsters exploit the less regulated part-time work to attract vulnerable job seekers.

**Required experience analysis**

```{r fig.width=10, fig.height=5}
exp_stats <- data %>%
  filter(required_experience != "") %>%
  group_by(required_experience) %>%
  summarise(
    total = n(),
    fraudulent_count = sum(fraudulent),
    fraud_rate = mean(fraudulent),
    .groups = "drop"
  ) %>%
  arrange(desc(fraud_rate))

print(exp_stats)

ggplot(exp_stats, aes(x = reorder(required_experience, -fraud_rate), y = fraud_rate, fill = required_experience)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(fraud_rate * 100, 1), "%")), vjust = -0.5, size = 3) +
  labs(title = "Fraud Rate by Required Experience Level",
       subtitle = "Do scammers target inexperienced candidates?",
       x = "Required Experience",
       y = "Fraud Rate") +
  theme_minimal() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1))
```

The data clearly confirms that scammers target inexperienced candidates experienced roles like Associate (1.8%), Internship (2.6%), and Mid-Senior level (3%) positions have significantly lower fraud rates. The inverse relationship between required experience and fraud probability suggests that scammers exploit both desperate job seekers with no experience requirements and ambitious candidates seeking executive positions. This pattern indicates that positions requiring minimal qualifications or offering unrealistic senior roles are red flags, as legitimate employers recruiting for experienced professionals tend to implement more thorough screening procedures that deter fraudulent postings.

**Binary feature analysis**

```{r fig.width=12, fig.height=5}
binary_features <- data %>%
  summarise(
    telecommuting_fraud_rate_0 = mean(fraudulent[telecommuting == 0]),
    telecommuting_fraud_rate_1 = mean(fraudulent[telecommuting == 1]),
    logo_fraud_rate_0 = mean(fraudulent[has_company_logo == 0]),
    logo_fraud_rate_1 = mean(fraudulent[has_company_logo == 1]),
    questions_fraud_rate_0 = mean(fraudulent[has_questions == 0]),
    questions_fraud_rate_1 = mean(fraudulent[has_questions == 1])
  )

binary_df <- data.frame(
  Feature = rep(c("Telecommuting", "Has Company Logo", "Has Questions"), each = 2),
  Value = rep(c("No (0)", "Yes (1)"), 3),
  FraudRate = c(
    binary_features$telecommuting_fraud_rate_0, binary_features$telecommuting_fraud_rate_1,
    binary_features$logo_fraud_rate_0, binary_features$logo_fraud_rate_1,
    binary_features$questions_fraud_rate_0, binary_features$questions_fraud_rate_1
  )
)

ggplot(binary_df, aes(x = Feature, y = FraudRate, fill = Value)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = paste0(round(FraudRate * 100, 1), "%")),
            position = position_dodge(width = 0.9), vjust = -0.5, size = 3) +
  scale_fill_manual(values = c("No (0)" = "#e74c3c", "Yes (1)" = "#2ecc71")) +
  labs(title = "Fraud Rate by Binary Features",
       x = "Feature",
       y = "Fraud Rate",
       fill = "Value") +
  theme_minimal()
```

Postings WITHOUT company logos and WITHOUT screening questions have significantly higher fraud rates. The absence of professional credibility markers dramatically increases fraud risk, with postings lacking company logos showing a 15.9% fraud rate compared to just 2.0% for those with logos, and postings without screening questions showing 6.8% fraud versus 2.8% with questions. Interestingly, telecommuting positions show a reversed pattern where remote work opportunities (Yes=8.3%) have higher fraud rates than non-remote positions (No=4.7%), suggesting scammers exploit the appeal and reduced verification requirements of work from home arrangements.

**Summary of EDA:**

1.  Dataset is really imbalanced (\~95% real, \~5% fake).
2.  Salary (in US only to avoid converting currency) Real postings mean salary:

```{r}
round(mean(real_data_us$salary_mid, na.rm = TRUE), 0)

```

While fake postings mean salary:

```{r}
round(mean(fraudulent_data_us$salary_mid, na.rm = TRUE), 0)
```

We can see that mean of fraudulent salaries is much higher than real ones.

3.  Description length\
    Real postings mean length:

```{r}
round(mean(real_data$desc_length), 0)
```

Fake postings mean length:

```{r}
round(mean(fraudulent_data$desc_length), 0)
```

Here we can't see big difference.

4.  Company profile.\
    Real postings missing profile:

    ```{r}
    round(mean(real_data$empty_profile) * 100, 1)
    ```

    Fake postings missing profile:

    ```{r}
    round(mean(fraudulent_data$empty_profile) * 100, 1)
    ```

    We can clearly see that there are much more missing profiles in fraudulent posting.

5.  Binary features we can see from visualizations that:

    Postings without company logo have higher fraud rate\
    Postings without screening questions have higher fraud rate

**Data prepared for Hypothesis testing.**

The following variables are now ready for hypothesis testing:

```{r}
salary_fraud <- fraudulent_data %>% filter(!is.na(salary_mid) & is_us) %>% pull(salary_mid)
salary_real <- real_data %>% filter(!is.na(salary_mid) & is_us) %>% pull(salary_mid)

fraud_desc <- fraudulent_data %>% pull(desc_length)
real_desc <- real_data %>% pull(desc_length)

fraud_profiles <- fraudulent_data %>% pull(empty_profile)
real_profiles <- real_data %>% pull(empty_profile)

california_posts <- data %>% filter(is_california == TRUE)
other_posts <- data %>% filter(is_california == FALSE)

cat("- salary_fraud:", length(salary_fraud), "observations\n")
cat("- salary_real:", length(salary_real), "observations\n")
cat("- fraud_desc:", length(fraud_desc), "observations\n")
cat("- real_desc:", length(real_desc), "observations\n")
cat("- California posts:", nrow(california_posts), "\n")
cat("- Other location posts:", nrow(other_posts), "\n")

n_salary_fraud <- length(salary_fraud)
n_salary_real <- length(salary_real)
```

# Hypothesis Testing

Hypothesis 1. Mean Salary comparision (only in US)

Research Question: Do fraudulent job postings offer different salary ranges compared to real ones? (Analyzing US postings only to avoid currency issues.)

We test:

-   $H_0$: Mean salary of fraudulent postings equals mean salary of real postings.
-   $H_1$: Mean salary of fraudulent postings differs from mean salary of real postings.

$$
H_0: \mu_F = \mu_R, \qquad H_1: \mu_F \neq \mu_R
$$

**Assumptions:** We assume that salaries in both groups are independent and normally distributed with a common variance.

**Test Statistic:** We use the classical two-sample Student's t-test with pooled variance.

The pooled sample variance is: $$
S_p^2 = \frac{(n_F - 1)S_F^2 + (n_R - 1)S_R^2}{n_F + n_R - 2}
$$

The test statistic is: $$
T = \frac{\bar{X}_F - \bar{X}_R}{S_p \sqrt{\frac{1}{n_F} + \frac{1}{n_R}}}
$$

Under the null hypothesis: $$
T \sim t_{n_F + n_R - 2}
$$

```{r}
x_bar_F <- mean(salary_fraud)
x_bar_R <- mean(salary_real)

s2_F <- var(salary_fraud)
s2_R <- var(salary_real)

#variance
s2_p <- ((n_salary_fraud - 1) * s2_F + (n_salary_real - 1) * s2_R) / (n_salary_fraud + n_salary_real - 2)
s_p <- sqrt(s2_p)

# T-statistic
T_stat <- (x_bar_F - x_bar_R) / (s_p * sqrt(1/n_salary_fraud + 1/n_salary_real))

# df
df_t <- n_salary_fraud + n_salary_real - 2

# Two-sided p-value
p_value <- 2 * pt(-abs(T_stat), df = df_t)

cat("Sample sizes:\n")
cat("  Fraudulent (n_F):", n_salary_fraud, "\n")
cat("  Real (n_R):", n_salary_real, "\n\n")

cat("Sample means:\n")
cat("  Mean (Fraudulent):", round(x_bar_F, 2), "\n")
cat("  Mean (Real):", round(x_bar_R, 2), "\n\n")

cat("Sample variances:\n")
cat("  Var (Fraudulent):", round(s2_F, 2), "\n")
cat("  Var (Real):", round(s2_R, 2), "\n\n")

cat("Pooled SD:", round(s_p, 2), "\n")
cat("T-statistic:", round(T_stat, 4), "\n")
cat("Degrees of freedom:", df_t, "\n")
cat("P-value:", format(p_value, scientific = TRUE, digits = 4), "\n")
```

Let\`s now verify with built in test:

```{r}
t.test(salary_fraud, salary_real, var.equal = TRUE)
```

Decision

```{r}
alpha <- 0.05
if (p_value < alpha) {
  cat("Reject H0: Mean salary of fraudulent postings significantly differs from mean salary of real postings.\n")
} else {
  cat("Fail to reject H0: No significant difference in mean salary between fraudulent and real postings.\n")
}
```

Hypothesis 2. Description length comparision

**Research Question:** Is there a difference in description length between fake and real job postings?

We test two one-sided alternatives:

**Test 2a:** $$
H_0: \mu_F = \mu_R, \qquad H_1: \mu_F > \mu_R
$$ (Fraudulent postings have longer descriptions)

**Test 2b:** $$
H_0: \mu_F = \mu_R, \qquad H_1: \mu_F < \mu_R
$$ (Fraudulent postings have shorter descriptions)

**Test Statistic:** The same pooled-variance t-statistic is used: $$
T = \frac{\bar{L}_F - \bar{L}_R}{S_p \sqrt{\frac{1}{n_F} + \frac{1}{n_R}}}
$$

**Decision Rules:** $$
\text{Reject } H_0 \text{ if } T > t_{\alpha, n_F+n_R-2} \quad (\text{for } H_1: \mu_F > \mu_R)
$$ $$
\text{Reject } H_0 \text{ if } T < -t_{\alpha, n_F+n_R-2} \quad (\text{for } H_1: \mu_F < \mu_R)
$$\
Calculate sample statistics for description length

```{r}
mean_fraud_desc <- mean(fraud_desc, na.rm = TRUE)
mean_real_desc <- mean(real_desc, na.rm = TRUE)

var_fraud_desc <- var(fraud_desc, na.rm = TRUE)
var_real_desc <- var(real_desc, na.rm = TRUE)

# variance
pooled_var_desc <- ((n_fake - 1) * var_fraud_desc + (n_real - 1) * var_real_desc) / (n_fake + n_real - 2)
pooled_sd_desc <- sqrt(pooled_var_desc)

# T-statistic
T_desc <- (mean_fraud_desc - mean_real_desc) / (pooled_sd_desc * sqrt(1/n_fake + 1/n_real))

# df
df_desc <- n_fake + n_real - 2

# Critical value for one-sided test
alpha_desc <- 0.05
t_crit_desc <- qt(1 - alpha_desc, df = df_desc)

# p-values for one-sided tests
p_greater_desc <- 1 - pt(T_desc, df = df_desc)  # H1: mu_F > mu_R
p_less_desc <- pt(T_desc, df = df_desc)          # H1: mu_F < mu_R

cat("Sample sizes:\n")
cat("  Fraudulent (n_F):", n_fake, "\n")
cat("  Real (n_R):", n_real, "\n\n")

cat("Sample means:\n")
cat("  Mean (Fraudulent):", round(mean_fraud_desc, 2), "characters\n")
cat("  Mean (Real):", round(mean_real_desc, 2), "characters\n\n")

cat("Pooled SD:", round(pooled_sd_desc, 2), "\n")
cat("T-statistic:", round(T_desc, 4), "\n")
cat("Degrees of freedom:", df_desc, "\n")
cat("Critical value (one-sided, alpha=0.05):", round(t_crit_desc, 4), "\n\n")

cat("Test 2a: H1: mu_F > mu_R (fraudulent longer)\n")
cat("P-value:", format(p_greater_desc, scientific = TRUE, digits = 4), "\n")
cat("Reject H0:", T_desc > t_crit_desc, "\n\n")

cat("Test 2b: H1: mu_F < mu_R (fraudulent shorter)\n")
cat("P-value:", format(p_less_desc, scientific = TRUE, digits = 4), "\n")
cat("Reject H0:", T_desc < -t_crit_desc, "\n")
```

verify with built-in function

```{r}
cat("Test 2a (alternative: greater):\n")
t.test(fraud_desc, real_desc, var.equal = TRUE, alternative = "greater")
```

```{r}
cat("Test 2b (alternative: less):\n")
t.test(fraud_desc, real_desc, var.equal = TRUE, alternative = "less")
```

Decision with alpha 0,05:

```{r}

if (T_desc > t_crit_desc) {
  cat("Test 2a: Reject H0 - Fraudulent postings have significantly LONGER descriptions.\n")
} else if (T_desc < -t_crit_desc) {
  cat("Test 2b: Reject H0 - Fraudulent postings have significantly SHORTER descriptions.\n")
} else {
  cat("Fail to reject H0 - No significant difference in description length.\n")
}
```

hypothesis 3. empty company profile proportion

**Research Question:** Are fraudulent postings more likely to have missing company profiles?

We compare proportions of empty company profiles between real and fraudulent postings.

Let: $$
\hat{p}_R = \frac{X_R}{n_R}, \qquad \hat{p}_F = \frac{X_F}{n_F}
$$ where $X_R$ and $X_F$ are the numbers of postings with empty company profiles.

**Test 3a:** $$
H_0: p_R = p_F, \qquad H_1: p_R > p_F
$$ (Real postings have MORE empty profiles)

**Test 3b:** $$
H_0: p_R = p_F, \qquad H_1: p_R < p_F
$$ (Real postings have FEWER empty profiles, i.e., fraudulent have more)

**Test Statistic:** The pooled proportion under $H_0$ is: $$
\hat{p} = \frac{X_R + X_F}{n_R + n_F}
$$

The z-statistic is: $$
Z = \frac{\hat{p}_R - \hat{p}_F}{\sqrt{\hat{p}(1-\hat{p})\left(\frac{1}{n_R} + \frac{1}{n_F}\right)}}
$$

Under the null hypothesis: $$
Z \sim N(0, 1)
$$

```{r}
# empty profiles in each group
x_real_prof <- sum(real_profiles)
x_fraud_prof <- sum(fraud_profiles)

p_hat_real_prof <- x_real_prof / n_real
p_hat_fraud_prof <- x_fraud_prof / n_fake

# proportion
p_pooled_prof <- (x_real_prof + x_fraud_prof) / (n_real + n_fake)

# Z-statistic
Z_prof <- (p_hat_real_prof - p_hat_fraud_prof) / sqrt(p_pooled_prof * (1 - p_pooled_prof) * (1/n_real + 1/n_fake))

# Critical value for one-sided test
alpha_prof <- 0.05
z_crit_prof <- qnorm(1 - alpha_prof)

# p-values for one-sided tests
p_val_greater_prof <- 1 - pnorm(Z_prof)  # H1: p_R > p_F
p_val_less_prof <- pnorm(Z_prof)          # H1: p_R < p_F

cat("Empty profiles :\n")
cat("  Real:", x_real_prof, "/", n_real, "=", round(p_hat_real_prof * 100, 2), "%\n")
cat("  Fraudulent:", x_fraud_prof, "/", n_fake, "=", round(p_hat_fraud_prof * 100, 2), "%\n\n")

cat("Pooled proportion:", round(p_pooled_prof, 4), "\n")
cat("Z-statistic:", round(Z_prof, 4), "\n")
cat("Critical value (one-sided, alpha=0.05):", round(z_crit_prof, 4), "\n\n")

cat("Test 3a: H1: p_R > p_F (real have more empty profiles)\n")
cat("P-value:", format(p_val_greater_prof, scientific = TRUE, digits = 4), "\n")
cat("Reject H0:", Z_prof > z_crit_prof, "\n\n")

cat("Test 3b: H1: p_R < p_F (fraudulent have more empty profiles)\n")
cat("P-value:", format(p_val_less_prof, scientific = TRUE, digits = 4), "\n")
cat("Reject H0:", Z_prof < -z_crit_prof, "\n")
```

Verify with built-in prop.test function

```{r}
cat("Test 3a (alternative: greater):\n")
prop.test(x = c(x_real_prof, x_fraud_prof), n = c(n_real, n_fake), alternative = "greater", correct = FALSE)
```

```{r}
cat("Test 3b (alternative: less):\n")
prop.test(x = c(x_real_prof, x_fraud_prof), n = c(n_real, n_fake), alternative = "less", correct = FALSE)
```

Decision with a = 0,05

```{r}
if (Z_prof > z_crit_prof) {
  cat("Test 3a: Reject H0 - Real postings have significantly MORE empty profiles.\n")
} else if (Z_prof < -z_crit_prof) {
  cat("Test 3b: Reject H0 - Fraudulent postings have significantly MORE empty profiles.\n")
} else {
  cat("Fail to reject H0 - No significant difference in empty profile proportions.\n")
}
```

Hypothesis 4. California Location and Fraud Rate.

**Research Question:** Does the probability that a job posting is fraudulent change when the posting is located in California?

Let: - $p_{CA}$ = probability of fraud for California postings - $p_{Other}$ = probability of fraud for non-California postings

$$
H_0: p_{CA} = p_{Other}, \qquad H_1: p_{CA} > p_{Other}
$$

**Test Statistic:** $$
Z = \frac{\hat{p}_{CA} - \hat{p}_{Other}}{\sqrt{\hat{p}(1-\hat{p})\left(\frac{1}{n_{CA}} + \frac{1}{n_{Other}}\right)}}
$$

**Decision Rule:** $$
\text{Reject } H_0 \text{ if } Z > z_\alpha
$$

```{r}
# fraudulent postings by location
n_california <- nrow(california_posts)
n_other <- nrow(other_posts)

x_fraud_cal <- sum(california_posts$fraudulent == 1)
x_fraud_other <- sum(other_posts$fraudulent == 1)

# fraud rates
p_hat_cal <- x_fraud_cal / n_california
p_hat_other <- x_fraud_other / n_other

# Pooled proportion
p_pooled_loc <- (x_fraud_cal + x_fraud_other) / (n_california + n_other)

# Z-statistic
Z_loc <- (p_hat_cal - p_hat_other) / sqrt(p_pooled_loc * (1 - p_pooled_loc) * (1/n_california + 1/n_other))

# Critical value
alpha_loc <- 0.05
z_crit_loc <- qnorm(1 - alpha_loc)

# p-value for one-sided test (H1: p_CA > p_Other)
p_val_loc <- 1 - pnorm(Z_loc)

cat("Sample sizes:\n")
cat("  California postings:", n_california, "\n")
cat("  Other locations:", n_other, "\n\n")

cat("Fraudulent counts:\n")
cat("  California:", x_fraud_cal, "/", n_california, "=", round(p_hat_cal * 100, 2), "%\n")
cat("  Other:", x_fraud_other, "/", n_other, "=", round(p_hat_other * 100, 2), "%\n\n")

cat("Pooled proportion:", round(p_pooled_loc, 4), "\n")
cat("Z-statistic:", round(Z_loc, 4), "\n")
cat("Critical value (one-sided, alpha=0.05):", round(z_crit_loc, 4), "\n")
cat("P-value:", format(p_val_loc, scientific = TRUE, digits = 4), "\n")
```

Verify with built-in prop.test function

```{r}

prop.test(x = c(x_fraud_cal, x_fraud_other), n = c(n_california, n_other), alternative = "greater", correct = FALSE)
```

Decision with a = 0,05:

```{r}
if (Z_loc > z_crit_loc) {
  cat("Reject H0: California postings have significantly HIGHER fraud rate than other locations.\n")
} else {
  cat("Fail to reject H0: No significant difference in fraud rate between California and other locations.\n")
}
```

**Summary of hypotheses testing:**

Hypothesis 1: Mean Salary Comparison (Two-sided t-test): Failed to Reject $H_0$

Hypothesis 2: Description Length (One-sided t-tests): Reject $H_0$

Hypothesis 3: Empty Company Profile Proportion (Z-tests): Reject $H_0$

Hypothesis 4: California Fraud Rate (One-sided Z-test): Reject $H_0$

**Logistic Regression**

In the previous sections we compared means and proportions between real and fraudulent postings using classical parametric tests. We now move to a binary classifier - logistic regression, which allows us to model the probability that a posting is fraudulent as a function of several predictors simultaneously.

1.  **Model specification:**

Let:

-   $Y_i \in \{0, 1\}$ be the response for posting i, where $Y_i = 1$ if the posting is fraudulent and $Y_i = 0$ if it is real
-   $X_i = (x_{i1}, \ldots, x_{ip})$ be the vector of predictors for posting $i$ (e.g., salary range, length of description, missing company profile, location, required experience, etc.)
-   $\pi_i = P(Y_i = 1 | X_i)$ be the conditional probability that posting $i$ is fraudulent

The logistic regression model assumes that: $$
Y_i | X_i \sim \text{Bernoulli}(\pi_i)
$$

and the log-odd of fraud are a linear function of the predictors: $$
\log \frac{\pi_i}{1 - \pi_i} = \eta_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}
$$

Equivalently, the conditional probability has the form: $$
\pi_i = P(Y_i = 1 | X_i) = \frac{\exp(\eta_i)}{1 + \exp(\eta_i)}
$$

For each coefficient $\beta_j$ ($j = 1, \ldots, p$), the quantity $\exp(\beta_j)$ is the likelihood odd ratio associated with a one-unit increase in $x_{ij}$, holding all other predictors fixed.

2.  **Estimation**

The parameters $\beta = (\beta_0, \ldots, \beta_p)$ are estimated by **maximum likelihood**. The likelihood function is: $$
L(\beta) = \prod_{i=1}^{n} \pi_i^{Y_i} (1 - \pi_i)^{1-Y_i}
$$

where $\pi_i = \pi_i(\beta)$ depends on $\beta$ through the logistic link.

We typically work with the log-likelihood: $$
\ell(\beta) = \sum_{i=1}^{n} [Y_i \log(\pi_i) + (1 - Y_i) \log(1 - \pi_i)]
$$

There is no closed-form solution for $\hat{\beta}$, so numerical optimization (iteratively reweighted least squares) is used to obtain the maximum likelihood estimator (MLE) $\hat{\beta}$ and its estimated covariance matrix $\widehat{\text{Var}}(\hat{\beta})$.

-   **Likelihood Ratio Tests for Individual Coefficients**

$$
\text{Reject } H_0 \text{ if } Z_j < -z_{1-\alpha} \quad \text{for } H_1: \beta_j < 0
$$

## Prepare Data for Logistic Regression

```{r}
model_data <- data %>%
  filter(is_us) %>%
  mutate(
    salary_standardized = ifelse(is.na(salary_mid), median(salary_mid, na.rm = TRUE), salary_mid),
    salary_standardized = scale(salary_standardized)[,1],
    desc_length_standardized = scale(desc_length)[,1],
    missing_profile = as.integer(empty_profile),
    entry_level = as.integer(required_experience %in% c("Entry level", "Not Applicable", "Internship", "")),
    has_logo = has_company_logo,
    has_questions = has_questions,
    telecommuting = telecommuting,
    california = as.integer(is_california)
  ) %>%
  select(fraudulent, salary_standardized, desc_length_standardized, missing_profile,
         entry_level, has_logo, has_questions, telecommuting, california)

cat("Observations:", nrow(model_data), "\n")
cat("Variables:", ncol(model_data), "\n\n")
str(model_data)
```

**Fit logistic regression model**

```{r}
logit_model <- glm(fraudulent ~ salary_standardized + desc_length_standardized +
                     missing_profile + entry_level + has_logo + has_questions +
                     telecommuting + california,
                   data = model_data,
                   family = binomial(link = "logit"))

summary(logit_model)
```

**Interpretation of coefficients**

```{r}
coef_table <- summary(logit_model)$coefficients
coef_df <- as.data.frame(coef_table)
colnames(coef_df) <- c("Estimate", "Std.Error", "z.value", "p.value")
coef_df$OddsRatio <- exp(coef_df$Estimate)

conf_int <- confint.default(logit_model)
coef_df$OR_CI_Lower <- exp(conf_int[,1])
coef_df$OR_CI_Upper <- exp(conf_int[,2])
coef_df$Significant <- ifelse(coef_df$p.value < 0.001, "***",
                        ifelse(coef_df$p.value < 0.01, "**",
                        ifelse(coef_df$p.value < 0.05, "*",
                        ifelse(coef_df$p.value < 0.1, ".", ""))))

coef_df_print <- coef_df
coef_df_print[, 1:7] <- round(coef_df_print[, 1:7], 4)
print(coef_df_print)
```

**Odd ratios with 95% CI**

```{r}
for (i in 1:nrow(coef_df)) {
  var_name <- rownames(coef_df)[i]
  or <- round(coef_df$OddsRatio[i], 4)
  ci_low <- round(coef_df$OR_CI_Lower[i], 4)
  ci_high <- round(coef_df$OR_CI_Upper[i], 4)
  sig <- coef_df$Significant[i]

  cat(sprintf("%-25s OR = %7.4f  [%7.4f, %7.4f] %s\n",
              var_name, or, ci_low, ci_high, sig))
}
```

**Hypotheses about predictors**

Based on the interim report, we test the following directional hypotheses:

1.  **Salary level**

    We expect unrealistically high salaries to be associated with a higher probability of fraud. $$
    H_0: \beta_{sal} = 0 \quad \text{vs.} \quad H_1: \beta_{sal} > 0
    $$

```{r}
beta_sal <- coef(logit_model)["salary_standardized"]
se_sal <- summary(logit_model)$coefficients["salary_standardized", "Std. Error"]
z_sal <- beta_sal / se_sal
p_sal_greater <- 1 - pnorm(z_sal)

cat("Coefficient:", round(beta_sal, 4), "\n")
cat("Std. Error:", round(se_sal, 4), "\n")
cat("Z-statistic:", round(z_sal, 4), "\n")
cat("P-value (one-sided):", format(p_sal_greater, scientific = TRUE, digits = 4), "\n")
cat("Odds Ratio:", round(exp(beta_sal), 4), "\n\n")

if (p_sal_greater < 0.05) {
cat("Result: REJECT H0 - Higher salary significantly increases fraud odds.\n")
} else {
cat("Result: FAIL TO REJECT H0 - No significant effect of salary on fraud.\n")
}
```

2.  **missing company profile**

    Postings with missing company profiles are expected to be more likely to be fake. $$
    H_0: \beta_{prof} = 0 \quad \text{vs.} \quad H_1: \beta_{prof} > 0
    $$

```{r}
beta_prof <- coef(logit_model)["missing_profile"]
se_prof <- summary(logit_model)$coefficients["missing_profile", "Std. Error"]
z_prof_logit <- beta_prof / se_prof
p_prof_greater <- 1 - pnorm(z_prof_logit)

cat("Coefficient:", round(beta_prof, 4), "\n")
cat("Std. Error:", round(se_prof, 4), "\n")
cat("Z-statistic:", round(z_prof_logit, 4), "\n")
cat("P-value (one-sided):", format(p_prof_greater, scientific = TRUE, digits = 4), "\n")
cat("Odds Ratio:", round(exp(beta_prof), 4), "\n\n")

if (p_prof_greater < 0.05) {
  cat("Result: REJECT H0 - Missing profile significantly increases fraud odds.\n")
  cat("Interpretation: Postings with missing company profile have",
      round(exp(beta_prof), 2), "times the odds of being fraudulent.\n")
} else {
  cat("Result: FAIL TO REJECT H0 - No significant effect of missing profile on fraud.\n")
}

```

3.  **Length of the description**

    Extremely short descriptions are expected to be more suspicious, so longer descriptions may decrease fraud probability. $$
    H_0: \beta_{len} = 0 \quad \text{vs.} \quad H_1: \beta_{len} < 0
    $$

```{r}
beta_len <- coef(logit_model)["desc_length_standardized"]
se_len <- summary(logit_model)$coefficients["desc_length_standardized", "Std. Error"]
z_len <- beta_len / se_len
p_len_less <- pnorm(z_len)

cat("Coefficient:", round(beta_len, 4), "\n")
cat("Std. Error:", round(se_len, 4), "\n")
cat("Z-statistic:", round(z_len, 4), "\n")
cat("P-value (one-sided):", format(p_len_less, scientific = TRUE, digits = 4), "\n")
cat("Odds Ratio:", round(exp(beta_len), 4), "\n\n")

if (p_len_less < 0.05) {
  cat("Result: REJECT H0 - Longer description significantly decreases fraud odds.\n")
  cat("Interpretation: Each 1 SD increase in description length multiplies\n")
  cat("               the odds of fraud by", round(exp(beta_len), 4), "\n")
} else {
  cat("Result: FAIL TO REJECT H0 - No significant effect of description length on fraud.\n")
}
```

4.  **Entry level experience**

    Scammers may target inexperienced candidates. $$
    H_0: \beta_{entry} = 0 \quad \text{vs.} \quad H_1: \beta_{entry} > 0
    $$

```{r}
beta_entry <- coef(logit_model)["entry_level"]
se_entry <- summary(logit_model)$coefficients["entry_level", "Std. Error"]
z_entry <- beta_entry / se_entry
p_entry_greater <- 1 - pnorm(z_entry)

cat("Coefficient:", round(beta_entry, 4), "\n")
cat("Std. Error:", round(se_entry, 4), "\n")
cat("Z-statistic:", round(z_entry, 4), "\n")
cat("P-value (one-sided):", format(p_entry_greater, scientific = TRUE, digits = 4), "\n")
cat("Odds Ratio:", round(exp(beta_entry), 4), "\n\n")

if (p_entry_greater < 0.05) {
  cat("Result: REJECT H0 - Entry-level positions have significantly higher fraud odds.\n")
} else {
  cat("Result: FAIL TO REJECT H0 - No significant effect of experience level on fraud.\n")
}

```

```{r}
cat("Log-Likelihood:", round(logLik(logit_model)[1], 2), "\n")

cat("Null Deviance:", round(logit_model$null.deviance, 2), "on", logit_model$df.null, "df\n")
cat("Residual Deviance:", round(logit_model$deviance, 2), "on", logit_model$df.residual, "df\n\n")

null_model <- glm(fraudulent ~ 1, data = model_data, family = binomial)
mcfadden_r2 <- 1 - (logLik(logit_model)[1] / logLik(null_model)[1])
cat("R-squared of logistic regression:", round(mcfadden_r2, 4), "\n")
```

$H_0$: All coefficients = 0 (model without predictors)

$H_1$: At least one coefficient != 0

```{r}
lr_stat <- logit_model$null.deviance - logit_model$deviance
lr_df <- logit_model$df.null - logit_model$df.residual
lr_pvalue <- 1 - pchisq(lr_stat, df = lr_df)


cat("LR Statistic:", round(lr_stat, 2), "\n")
cat("P-value:", format(lr_pvalue, scientific = TRUE, digits = 4), "\n\n")

if (lr_pvalue < 0.05) {
  cat("Result: REJECT H0 - The model is significantly better than model without predictors.\n")
} else {
  cat("Result: FAIL TO REJECT H0 - The model is not significantly better than the model without predictors.\n")
}
```

**Prediction Performance**

### 

Find optimal threshold by optimizing F1 score

```{r}
model_data$predicted_prob <- predict(logit_model, type = "response")

thresholds <- seq(0.01, 0.99, by = 0.01)
f1_scores <- sapply(thresholds, function(t) {
  pred <- ifelse(model_data$predicted_prob > t, 1, 0)

  TP <- sum(pred == 1 & model_data$fraudulent == 1)
  FP <- sum(pred == 1 & model_data$fraudulent == 0)
  FN <- sum(pred == 0 & model_data$fraudulent == 1)

  precision <- ifelse((TP + FP) > 0, TP / (TP + FP), 0)
  recall <- ifelse((TP + FN) > 0, TP / (TP + FN), 0)
  f1 <- ifelse((precision + recall) > 0, 2 * precision * recall / (precision + recall), 0)

  return(f1)
})

optimal_threshold <- thresholds[which.max(f1_scores)]
max_f1 <- max(f1_scores)

cat("Optimal threshold (maximizing F1):", round(optimal_threshold, 3), "\n")
cat("F1 score at optimal threshold:", round(max_f1, 4), "\n\n")
```

```{r fig.width=10, fig.height=5}
threshold_df <- data.frame(threshold = thresholds, f1_score = f1_scores)

ggplot(threshold_df, aes(x = threshold, y = f1_score)) +
  geom_line(color = "#3498db", size = 1) +
  geom_vline(xintercept = optimal_threshold, linetype = "dashed", color = "#e74c3c") +
  geom_vline(xintercept = 0.5, linetype = "dotted", color = "gray") +
  annotate("text", x = optimal_threshold, y = max_f1 * 0.95,
           label = paste0("Optimal: ", round(optimal_threshold, 3)),
           hjust = -0.1, color = "#e74c3c") +
  annotate("text", x = 0.5, y = max_f1 * 0.9,
           label = "Default: 0.5", hjust = 1.1, color = "gray") +
  labs(title = "F1 Score vs Classification Threshold",
       x = "Threshold",
       y = "F1 Score") +
  theme_minimal()
```

Performance at optimal threshold

```{r}

calculate_metrics <- function(actual, predicted) {
  TP <- sum(predicted == 1 & actual == 1)
  TN <- sum(predicted == 0 & actual == 0)
  FP <- sum(predicted == 1 & actual == 0)
  FN <- sum(predicted == 0 & actual == 1)

  accuracy <- (TP + TN) / length(actual)
  precision <- ifelse((TP + FP) > 0, TP / (TP + FP), 0)
  recall <- ifelse((TP + FN) > 0, TP / (TP + FN), 0)
  specificity <- ifelse((TN + FP) > 0, TN / (TN + FP), 0)
  f1 <- ifelse((precision + recall) > 0, 2 * precision * recall / (precision + recall), 0)

  return(list(
    confusion = matrix(c(TN, FP, FN, TP), nrow = 2,
                      dimnames = list(Actual = c("0", "1"), Predicted = c("0", "1"))),
    TP = TP, TN = TN, FP = FP, FN = FN,
    accuracy = accuracy, precision = precision,
    recall = recall, specificity = specificity, f1 = f1
  ))
}

pred_optimal <- ifelse(model_data$predicted_prob > optimal_threshold, 1, 0)
metrics_optimal <- calculate_metrics(model_data$fraudulent, pred_optimal)
pred_default <- ifelse(model_data$predicted_prob > 0.5, 1, 0)
metrics_default <- calculate_metrics(model_data$fraudulent, pred_default)

cat("Confusion Matrix (threshold = ", round(optimal_threshold, 3), ")\n\n", sep = "")
print(metrics_optimal$confusion)

cat("Accuracy:", round(metrics_optimal$accuracy * 100, 2), "%\n")
cat("Precision:", round(metrics_optimal$precision * 100, 2), "%\n")
cat("Recall (Sensitivity):", round(metrics_optimal$recall * 100, 2), "%\n")
cat("Specificity:", round(metrics_optimal$specificity * 100, 2), "%\n")
cat("F1 Score:", round(metrics_optimal$f1, 4), "\n")

cat("Comparison with Default Threshold specifically (0.5)\n\n")
cat("Accuracy:\n")
cat("Optimal: ", metrics_optimal$accuracy *100, "Default: ",  metrics_default$accuracy *100,"\n")
cat("Precision:\n")
cat("Optimal: ", metrics_optimal$precision * 100, "Default: ",  metrics_default$precision * 100,"\n")
cat("Recall:\n")
cat("Optimal: ", metrics_optimal$recall * 100, "Default: ",  metrics_default$recall * 100,"\n")
cat("Specificity:\n")
cat("Optimal: ",metrics_optimal$specificity * 100, "Default: ",  metrics_optimal$specificity * 100,"\n")
cat("F1 Score:\n")
cat("Optimal: ", metrics_optimal$f1 * 100, "Default: ",  metrics_optimal$f1 * 100,"\n")

accuracy <- metrics_optimal$accuracy
precision <- metrics_optimal$precision
recall <- metrics_optimal$recall
f1_score <- metrics_optimal$f1
```

We can see comparing to the default metrics that our model performs really poorly using default threshold: Precision 0, F1 0 and so on. That happens because of the imbalance in our dataset. Key insights: optimal threshold balances precision and recall for fraud detection, lower threshold increases recall (catches more frauds) at cost of some precision for fraud detection, higher recall is often preferred to minimize false negatives.

**Receiver Operating Characteristic curve and Area Under the Curve**

```{r fig.width=10, fig.height=5}
thresholds <- seq(0, 1, by = 0.01)
roc_data <- data.frame(
  threshold = thresholds,
  TPR = sapply(thresholds, function(t) {
    pred <- ifelse(model_data$predicted_prob > t, 1, 0)
    sum(pred == 1 & model_data$fraudulent == 1) / sum(model_data$fraudulent == 1)
  }),
  FPR = sapply(thresholds, function(t) {
    pred <- ifelse(model_data$predicted_prob > t, 1, 0)
    sum(pred == 1 & model_data$fraudulent == 0) / sum(model_data$fraudulent == 0)
  })
)

roc_data <- roc_data[order(roc_data$FPR), ]
auc <- sum(diff(roc_data$FPR) * (head(roc_data$TPR, -1) + tail(roc_data$TPR, -1)) / 2)

optimal_point <- roc_data[which.min(abs(roc_data$threshold - optimal_threshold)), ]

ggplot(roc_data, aes(x = FPR, y = TPR)) +
  geom_line(color = "#e74c3c", size = 1.2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
  geom_point(data = optimal_point, aes(x = FPR, y = TPR),
             color = "#2ecc71", size = 4) +
  annotate("text", x = 0.7, y = 0.3, label = paste0("AUC = ", round(abs(auc), 4)), size = 5) +
  annotate("text", x = optimal_point$FPR, y = optimal_point$TPR,
           label = paste0("Optimal\n(t=", round(optimal_threshold, 3), ")"),
           hjust = -0.2, vjust = -0.5, color = "#2ecc71", size = 3.5) +
  labs(title = "ROC Curve for Logistic Regression Model",
       x = "False Positive Rate (1 - Specificity)",
       y = "True Positive Rate (Sensitivity)") +
  theme_minimal() +
  coord_equal()

cat("\nAUC:", round(abs(auc), 4), "\n")
```

Summary:

```{r}
cat("R-squared for logistic regression:", round(mcfadden_r2, 4), "\n")
cat("Model is", ifelse(lr_pvalue < 0.05, "significant", "not significant"),
    "(LR test p-value:", format(lr_pvalue, scientific = TRUE, digits = 4), ")\n\n")

cat("significant predictors (alpha = 0.05):\n")
sig_predictors <- rownames(coef_df)[coef_df$p.value < 0.05 & rownames(coef_df) != "(Intercept)"]
if (length(sig_predictors) > 0) {
  for (pred in sig_predictors) {
    or <- round(coef_df[pred, "OddsRatio"], 4)
    direction <- ifelse(coef_df[pred, "Estimate"] > 0, "increases", "decreases")
    cat("  -", pred, ": OR =", or, "(", direction, "fraud odds)\n")
  }
} else {
  cat("  None\n")
}

cat("prediction performance (at optimal threshold =", round(optimal_threshold, 3), "):\n")
cat("  Accuracy:", round(accuracy * 100, 2), "%\n")
cat("  Precision:", round(precision * 100, 2), "%\n")
cat("  Recall:", round(recall * 100, 2), "%\n")
cat("  F1 Score:", round(f1_score, 4), "\n")
cat("  AUC:", round(abs(auc), 4), "\n")

cat("Hypotheses about predictors:\n")
cat("Salary -> fraud:", ifelse(p_sal_greater < 0.05, "SUPPORTED", "NOT SUPPORTED"), "\n")
cat("Missing profile -> fraud:", ifelse(p_prof_greater < 0.05, "SUPPORTED", "NOT SUPPORTED"), "\n")
cat("Longer desc -> less fraud:", ifelse(p_len_less < 0.05, "SUPPORTED", "NOT SUPPORTED"), "\n")
cat("Entry-level -> fraud:", ifelse(p_entry_greater < 0.05, "SUPPORTED", "NOT SUPPORTED"), "\n")
```

**Conclusion**

This analysis identified several predictors of fraudulent job postings in a highly imbalanced dataset. The logistic regression model achieved an AUC of 0.86, indicating good discriminatory power, though the R of 0.17 suggests the model explains only 17% of fraud variance.

The strongest fraud indicators are missing company profiles, absence of company logos, and lack of screening questions. Entry-level positions and telecommuting roles show elevated fraud risk, confirming that scammers target vulnerable, inexperienced job seekers.

Contrary to initial hypotheses, salary level does not significantly predict fraud, despite fraudulent postings having a higher mean salary. This apparent contradiction reflects fraud's U-shaped salary distribution: scammers employ strategies, offering both unrealistically low wages to desperate candidates and extremely high to lure ambitious applicants.

Model Performance and Limitations: At the optimized threshold, the model achieves 51.92% recall and 42% precision, meaning 58% of fraud flags are false positives. While this aggressive threshold maximizes fraud detection, false positives may deter legitimate job seekers, while false negatives expose vulnerable individuals to scams. The moderate F1 score reflects the difficulty of detecting rare events in highly imbalanced dataset.

Job platforms should implement automated screening that flags postings lacking company profiles, logos, or screening questions -these features collectively provide a reliable fraud signal. However, statistical models alone cannot solve this problem; the 17% explained variance indicates that unobserved factors likely play crucial roles.
